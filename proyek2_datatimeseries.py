# -*- coding: utf-8 -*-
"""Proyek2_DataTimeSeries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OU5L_oBDTWgvpl4JYTOXbxRPYhKzepCq

Arindita Prihastama | 1494037162100-370 | arinditap@gmail.com
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.layers import Dense, LSTM, Bidirectional, Dropout
from keras.models import Sequential

url = 'https://raw.githubusercontent.com/arinditap/dicoding-MachineLearning/main/testset.csv'
dt = pd.read_csv(url)
dt.head()

dt.isnull().sum()

df = dt[['datetime_utc',' _tempm']]
df

#df.columns = ['date', 'temp']

df.isnull().sum()

df.dropna(inplace = True)

df['datetime_utc'] = pd.to_datetime(df['datetime_utc'])
df.info()
df.head()

df = df.set_index('datetime_utc')

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
data = scaler.fit_transform(df)

plt.plot(data)

tgl = df['datetime_utc'].values
temp = df[' _tempm'].values



latih_size = int(len(data) * 0.8)
uji_size = len(data) - latih_size
X_data, y_data = data[0:latih_size, :], data[latih_size: len(data), :1]
latih_size, uji_size

def dataset(dataset, time_step=1):
  X, Y = [],[]
  for i in range(len(dataset) - time_step - 1):
    a = dataset[i:(i+time_step), 0]
    X.append(a)
    Y.append(dataset[i + time_step, 0])
  return np.array(X), np.array(Y)

X_train, y_train = dataset(X_data, 100)
X_test, y_test = dataset(y_data, 100)

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(64, input_shape = (100,1), return_sequences = True),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.LSTM(64, return_sequences = True),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(1),
])

skala_data = (max(data) - min(data)) * 0.1
print(skala_data)

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('mae') < skala_data) and (logs.get('val_mae') < skala_data):
            self.model.stop_training = True;
            print('\nMae telah dibawah 10% dari skala data yaitu {}'.format(skala_data))

callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(lr=1.000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=['mae'])
history = model.fit(
    X_train, y_train,
    validation_data = (X_test, y_test),
    epochs = 5,
    batch_size = 128, 
    callbacks = [callbacks],
    verbose = 1
)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'], loc = 'upper right')
plt.show()

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model MAE')
plt.ylabel('MAE')
plt.xlabel('Epochs')
plt.legend(['train', 'test'], loc = 'upper right')
plt.show()